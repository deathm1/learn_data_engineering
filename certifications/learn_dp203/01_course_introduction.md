# Introduction

There has been an exponential increase in the number of devices and software that generate data to meet current business and user needs. Businesses, Store, Interpret, Manage, Transform, Process, Aggregate and report on this data to internal management investors, business partners, regulators, consumers and other stakeholders.

Data consumers view data on PCs, tablets and mobile devices that are either connected or disconnected. Consumers generate and use data. They do this at work during leisure time and with social media applications. Business stakeholders use data to make business decisions, consumers use data to make personal decisions, for example what to buy.
Thanks to AI, Azure machine learning can now both consume data and make decisions the way humans do.
Data formats include text stream, audio, video, metadata.

Data can be structured, unstructured or aggregated.

# Structured Database Model

For structured databases, data architects define the structure schema as they create the data storage in platform technologies such as:

- Azure SQL database
- Azure SQL data warehouse (Azure Synapse Analytics)

Structured data can be stored in:

- Azure Blob storage
- Azure SQL Database
- Azure SQL Data Warehouse (Azure Synapse Analytics)

# Unstructured Model

For unstructured, no SQL databases, each data element can have it's own schema at query time. Data can be stored as:

- a file in Azure blob storage
- no SQL data in Azure Cosmos DB or Azure HD insight.

# Data Compliance

Data engineers must maintain data systems that are accurate, highly secure and constantly available. The systems must comply with applicable regulations such as

- GDPR: General Data Protection Regulation
- PCIDSS: Payment Card Industry Data Security Standard

International companies have special data requirements to conform with regional norms such as local languages and date formats.

Microsoft Azure provides a comprehensive and rich set of data technologies that can:

1. securely store
2. transform
3. process
4. analyze
5. visualize

a variety of data formats.

As data formats evolve, Microsoft releases new technologies to the Azure platform as your customers can explore these in preview mode.

Using the on-demand Azure subscription model, customers minimize costs, paying only for what they consume and only when they need it.

# On-Premises and Cloud-Based Servers

| Aspect                        | On-Premises Environments                                                                                                                                                            | Cloud Environments                                                                                                                                              |
| ----------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Computing Environment         | - Requires equipment to execute applications and services, including servers, infrastructure, and storage.                                                                          | - Provides physical and logical infrastructure to host services, virtual servers, intelligent applications, and containers without capital investment.          |
|                               | - Needs power, cooling, and maintenance.                                                                                                                                            | - Organizations pay only for what they use, reducing capital expenditure.                                                                                       |
|                               | - Requires at least one operating system (OS) per server, with potential use of virtualization technology.                                                                          | - Provisioning services can be done within minutes using Microsoft Azure services.                                                                              |
| Licensing                     | - Each OS installed on a server may have a licensing cost, typically sold per server or per Client Access License (CAL).                                                            |                                                                                                                                                                 |
|                               | - Licensing becomes more restrictive as companies grow.                                                                                                                             |                                                                                                                                                                 |
| Maintenance                   | - Requires maintenance for hardware, firmware, drivers, BIOS, operating systems, software, and antivirus software.                                                                  | - Microsoft manages key infrastructure services, including hardware, networking, firewalls, security, data center fault tolerance, and compliance.              |
|                               |                                                                                                                                                                                     | - Updates to operating systems and firmware are handled by Microsoft.                                                                                           |
| Scalability                   | - Can scale up (increase resources on a server) or scale out (add another server to a cluster).                                                                                     | - Scalability can be achieved easily and quickly, often with a mouse click, and is measured in compute units.                                                   |
|                               | - Server clustering requires identical hardware for each server in the cluster.                                                                                                     |                                                                                                                                                                 |
|                               | - Reaching maximum capacity in a cluster requires replacing or upgrading each node.                                                                                                 |                                                                                                                                                                 |
| Availability                  | - High availability systems must meet Service-Level Agreements (SLAs) specifying availability expectations (e.g., 99.9%, 99.99%, 99.999%).                                          | - Azure duplicates customer content for redundancy and high availability, with SLAs to ensure customers understand platform capabilities.                       |
|                               | - Higher uptime requirements increase costs.                                                                                                                                        |                                                                                                                                                                 |
| Support                       | - Hundreds of vendors for physical server hardware mean administrators need diverse platform knowledge.                                                                             | - Cloud systems have standardized environments, simplifying support and updates.                                                                                |
|                               | - Difficulty in finding qualified server administrators.                                                                                                                            |                                                                                                                                                                 |
| Multilingual Support          | - Difficult and expensive in on-premises SQL Server systems due to varying sorting orders of text data in different languages.                                                      | - Often stores data as JSON files with language code identifiers (LCID), using translation services like Microsoft Bing Translator API for language conversion. |
|                               | - Requires configuring collation settings and designing systems with multilingual functionality.                                                                                    |                                                                                                                                                                 |
| Total Cost of Ownership (TCO) | - Includes costs such as hardware, software licensing, labor, installation, upgrades, maintenance, data center overhead, power, telecommunications, building, heating, and cooling. | - Costs are tracked by subscriptions based on usage (compute unitsâ€™ hours or transactions), including hardware, software, disk storage, and labor.              |
|                               | - Difficult to align expenses with actual usage, often resulting in excess capacity.                                                                                                | - Typically aligns more closely with actual usage, though underutilization can still occur.                                                                     |
|                               | - Costs are capitalized and spread over the equipment's lifetime, limiting the ability to upgrade.                                                                                  | - Expenses are recorded monthly, providing flexibility to upgrade as needed.                                                                                    |
| Lift and Shift                | -                                                                                                                                                                                   | - Migrating applications from on-premises to Azure Virtual Machines without re-architecting.                                                                    |
|                               | -                                                                                                                                                                                   | - Provides immediate benefits like higher availability and lower operational costs, though existing applications may not leverage all Azure features.           |

# ETL (Extract Transform Load) vs ELT (Extract Load Transform)

| Aspect                      | ETL (Extract, Transform, Load)                                                                                                                                                                                                                                                        | ELT (Extract, Load, Transform)                                                                                                                                                                                                                                            |
| --------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Process Flow**            | 1. **Extract:** Data is extracted from various sources.                                                                                                                                                                                                                               | 1. **Extract:** Data is extracted from various sources.                                                                                                                                                                                                                   |
|                             | 2. **Transform:** Data is transformed (cleaned, aggregated, and processed) in a staging area or intermediate server.                                                                                                                                                                  | 2. **Load:** Raw data is loaded into the target data warehouse or data lake.                                                                                                                                                                                              |
|                             | 3. **Load:** Transformed data is loaded into the target data warehouse or data mart.                                                                                                                                                                                                  | 3. **Transform:** Data is transformed within the data warehouse using its processing capabilities.                                                                                                                                                                        |
| **Transformation Location** | Outside the data warehouse, typically in a staging area or intermediary server.                                                                                                                                                                                                       | Inside the data warehouse, utilizing its processing power.                                                                                                                                                                                                                |
| **Data Handling**           | Data is transformed before it is loaded into the data warehouse.                                                                                                                                                                                                                      | Raw data is loaded into the data warehouse first and transformed as needed.                                                                                                                                                                                               |
| **Performance**             | May be slower for large datasets as transformations are done before loading.                                                                                                                                                                                                          | Generally faster for large datasets as modern data warehouses can efficiently handle transformations.                                                                                                                                                                     |
| **Scalability**             | Less scalable, as the staging area may become a bottleneck with increasing data volume.                                                                                                                                                                                               | More scalable, leveraging the scalability of cloud-based data warehouses like Azure Synapse, Snowflake, etc.                                                                                                                                                              |
| **Complexity**              | ETL tools often require more setup and management of intermediate storage and compute resources.                                                                                                                                                                                      | ELT simplifies architecture by reducing the need for intermediate staging and separate transformation processes.                                                                                                                                                          |
| **Data Freshness**          | Typically involves batch processing, which may result in less frequent data updates.                                                                                                                                                                                                  | Supports more real-time or near-real-time data processing, allowing fresher data.                                                                                                                                                                                         |
| **Use Case Suitability**    | Suitable for environments where data transformations are complex and require specific business rules to be applied.                                                                                                                                                                   | Ideal for environments with large volumes of data and where data processing power is essential for transformations.                                                                                                                                                       |
| **Tools**                   | Common ETL tools include Informatica, Talend, and Apache Nifi.                                                                                                                                                                                                                        | Common ELT tools include Azure Data Factory, Google BigQuery, and AWS Glue.                                                                                                                                                                                               |
| **Cost**                    | May involve higher costs due to the need for intermediate storage and additional compute resources.                                                                                                                                                                                   | Potentially lower costs by utilizing the built-in capabilities of cloud data warehouses for transformations.                                                                                                                                                              |
| **Maintenance**             | Requires ongoing maintenance of ETL processes, including managing staging areas and transformation logic.                                                                                                                                                                             | Easier to maintain due to simpler architecture and leveraging data warehouse capabilities for transformations.                                                                                                                                                            |
| **Data Governance**         | Easier to enforce data governance and quality before loading data into the warehouse.                                                                                                                                                                                                 | Data governance and quality control are enforced within the data warehouse, often using built-in features.                                                                                                                                                                |
| **Example Scenarios**       | - Financial data integration where complex transformations are required before analysis.                                                                                                                                                                                              | - Big data analytics where large volumes of raw data need to be quickly loaded and transformed on-demand.                                                                                                                                                                 |
|                             | - Data migration from legacy systems to a new data warehouse.                                                                                                                                                                                                                         | - Real-time data processing for applications like IoT analytics and monitoring.                                                                                                                                                                                           |
| **How it works on Azure**   | - **Azure Data Factory (ADF):** Can be used to orchestrate ETL workflows, connecting to various data sources, transforming data using data flows or external processing engines, and loading data into Azure SQL Data Warehouse, Azure Synapse Analytics, or Azure Data Lake Storage. | - **Azure Data Factory (ADF):** Can be used to orchestrate ELT workflows, extracting data from various sources, loading raw data into Azure Synapse Analytics or Azure Data Lake Storage, and leveraging the compute power of these platforms to perform transformations. |
|                             | - **Azure Databricks:** Can be used for data transformations as part of ETL workflows, providing an environment for running Apache Spark jobs.                                                                                                                                        | - **Azure Synapse Analytics:** Provides powerful SQL-based data transformation capabilities within the data warehouse, enabling complex transformations on large datasets after loading.                                                                                  |
|                             | - **Azure SQL Database/Managed Instance:** Can be used as target storage for transformed data in ETL processes.                                                                                                                                                                       | - **Azure Data Lake Storage:** Serves as a storage layer for raw data, enabling subsequent transformations using Azure Synapse Analytics, Azure Databricks, or other tools.                                                                                               |

# Data Engineer vs Data Scientist vs AI Engineer

| Aspect                     | Data Engineer                                                                                                    | Data Scientist                                                                                       | AI Engineer                                                                                                        |
| -------------------------- | ---------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------ |
| **Primary Focus**          | Building and maintaining data infrastructure and pipelines.                                                      | Analyzing and interpreting complex data to derive insights and inform decision-making.               | Developing, deploying, and maintaining AI models and systems.                                                      |
| **Key Responsibilities**   | - Design, construct, and manage data pipelines.                                                                  | - Collect, analyze, and interpret large datasets.                                                    | - Design and implement machine learning and AI models.                                                             |
|                            | - Ensure data quality, integrity, and security.                                                                  | - Develop statistical models and algorithms to analyze data.                                         | - Integrate AI models into applications and systems.                                                               |
|                            | - Work with ETL processes to extract, transform, and load data.                                                  | - Use data visualization tools to present findings.                                                  | - Optimize AI models for performance and scalability.                                                              |
|                            | - Optimize data storage and retrieval.                                                                           | - Communicate insights and findings to stakeholders.                                                 | - Work with data scientists to translate models into production-ready solutions.                                   |
| **Skills Required**        | - Proficiency in SQL, NoSQL databases, and data warehousing solutions.                                           | - Strong statistical and mathematical skills.                                                        | - Strong understanding of machine learning algorithms and techniques.                                              |
|                            | - Knowledge of big data technologies like Hadoop, Spark, and Kafka.                                              | - Proficiency in programming languages like Python, R, and SQL.                                      | - Proficiency in programming languages like Python, R, Java, and C++.                                              |
|                            | - Experience with cloud platforms like AWS, Azure, and Google Cloud.                                             | - Experience with data visualization tools like Tableau, Power BI, and Matplotlib.                   | - Experience with deep learning frameworks like TensorFlow, PyTorch, and Keras.                                    |
|                            | - Proficiency in scripting languages like Python and Bash.                                                       | - Knowledge of machine learning techniques and tools.                                                | - Understanding of software engineering principles and best practices.                                             |
| **Tools and Technologies** | - Databases: SQL, NoSQL (e.g., MongoDB, Cassandra).                                                              | - Programming: Python, R, SQL.                                                                       | - Machine Learning: TensorFlow, PyTorch, Keras.                                                                    |
|                            | - Big Data: Hadoop, Spark, Kafka.                                                                                | - Visualization: Tableau, Power BI, Matplotlib.                                                      | - Development: Python, Java, C++.                                                                                  |
|                            | - Cloud: AWS, Azure, Google Cloud.                                                                               | - Libraries: Scikit-learn, Pandas, NumPy.                                                            | - Cloud: AWS SageMaker, Azure ML, Google AI Platform.                                                              |
|                            | - ETL Tools: Apache NiFi, Talend, Informatica.                                                                   | - Statistical Analysis: SPSS, SAS.                                                                   | - DevOps: Docker, Kubernetes, CI/CD tools.                                                                         |
| **Typical Deliverables**   | - Data pipelines and workflows.                                                                                  | - Data analysis reports and dashboards.                                                              | - Deployed AI models and APIs.                                                                                     |
|                            | - Data warehouses and lakes.                                                                                     | - Predictive models and insights.                                                                    | - Scalable AI solutions integrated with business applications.                                                     |
| **Collaboration**          | - Works closely with data scientists and business analysts to provide the necessary data infrastructure.         | - Collaborates with data engineers to ensure data is accessible and properly formatted for analysis. | - Works with data scientists to transition models from research to production.                                     |
|                            | - Engages with IT and operations teams to ensure data platform stability and performance.                        | - Communicates findings to business stakeholders to guide strategic decisions.                       | - Collaborates with software engineers to integrate AI models into applications.                                   |
| **Educational Background** | - Typically holds a degree in Computer Science, Engineering, or related fields.                                  | - Typically holds a degree in Data Science, Statistics, Mathematics, or related fields.              | - Typically holds a degree in Computer Science, AI, Machine Learning, or related fields.                           |
|                            | - Relevant certifications in cloud platforms and big data technologies.                                          | - Advanced degrees (Master's or Ph.D.) are common for specialized roles.                             | - Advanced degrees (Master's or Ph.D.) may be required for specialized roles.                                      |
| **Career Path**            | - Roles: Data Analyst, Data Architect, ETL Developer, Big Data Engineer.                                         | - Roles: Data Analyst, Research Scientist, Machine Learning Engineer, Business Analyst.              | - Roles: Machine Learning Engineer, AI Specialist, Robotics Engineer, Deep Learning Engineer.                      |
|                            | - Progression to: Senior Data Engineer, Data Engineering Manager, Chief Data Officer (CDO).                      | - Progression to: Senior Data Scientist, Data Science Manager, Chief Data Scientist.                 | - Progression to: Senior AI Engineer, AI Architect, Chief AI Officer (CAIO).                                       |
| **Impact on Business**     | - Ensures reliable, scalable, and efficient data infrastructure that supports business analytics and operations. | - Provides actionable insights and predictive models that inform business strategies and decisions.  | - Develops intelligent systems that automate processes and provide advanced capabilities to business applications. |


# Data storage in Azure Storage 

Azure storage accounts are the base storage type within Azure. Azure storage offers a very scalable object store for data objects and file system services in the cloud. It can also provide a messaging store for reliable messaging, or it can act as a no sequel store.  

Azure storage offers four configuration options:

- Azure blob, a scalable object store for text and binary data.  
- Azure files, managed file shares for cloud or on premises deployments.  
- Azure queue, a messaging store for reliable messaging between application components.  
- Azure table and no sequel store for no schema storage of structured data.  

You can use Azure storage as the storage basis when you are provisioning a data platform technology such as Azure Data Lake storage and HD insight. But you can also provisions Azure storage for standalone use for example, you provision an Azure blob store either as standard storage in the form of magnetic disk storage or is premium storage in the form of solid-state drives, SSDs. 

If you need to provision a data store that will store but not query data, your cheapest option is to set up a storage account as a blob store. Blob storage works well with images and unstructured data, and it's the cheapest way to store data in Azure. It also provides rest API and SDK for Azure storage in various languages. And supported code languages include .NET, java, node, python PHP ruby and go. Azure storage also supports scripting in Azure Power Shell and in the Azure command line interface. 

Data ingestion, to ingest data into your system use: 

- Azure Data factory storage Explorer 
- The AZ Copy tool 
- Power Shell or visual studio.  

If you use the file upload feature to import file sizes above two gigabytes, use Power Shell or Visual Studio.  

AZ Copy supports a maximum file size of one terabyte and automatically splits Data files that exceed 200 GB.  

If you create a storage account as a blob store, you can't query the data directly. To directly query the data, either move the data to a store that supports queries or set up the Azure storage account for a data lake storage. 

Azure storage encrypts all data that's written to it. Azure storage also provides you with fine grain control over who has access to your data. You'll secure the data by using keys or shared access signatures.  

Azure resource manager provides a permissions model that uses role-based access control, RBAC. Use this functionality to set permissions and assign roles to user groups or applications. 

Azure Data Lake Storage 

Azure Data Lake Storage is a Hadoop compatible data repository that can store any size or type of data. This storage service is available as Generation 1, Gen 1, or Generation 2, Gen 2. Data Lake Storage Gen 1 users don't have to upgrade to Gen 2, but without an upgrade, they do forego some benefits. Data Lake Storage is designed to store massive amounts of data for big data analytics. As an example, consider Contoso Life Sciences, Accounts and Research Center, they analyse petabytes of genetic data, patient data, and records of related sample data. Data Lake Storage Gen 2 reduces computation times, making the research faster and less expensive. The compute aspect that sits above this storage can vary. The aspect can include platforms like HDInsight, Hadoop, and Azure Databricks. Here are the key features of Data Lake Storage, unlimited scalability, Hadoop compatibility, security support for access-control lists or ACLs, POSIX compliance and optimized Azure Blob Filesystem, ABFS, driver that's designed for big data analytics, zone redundant storage and geo-redundant storage. 

 